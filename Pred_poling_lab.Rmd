---
title: 'ML #3 Predictive Policing'
author: "Michael Dunst and Sofia Fosullo"
date: "10/21/2022"
output: html_document
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE)

library(tidyverse)
library(ggplot2)
library(sf)
library(RSocrata)
library(viridis)
library(spatstat)
library(raster)
library(spdep)
library(FNN)
library(grid)
library(gridExtra)
library(knitr)
library(kableExtra)
library(tidycensus)
library(classInt)   # for KDE and ML risk class intervals
# functions
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"
source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")

```

## Read in Vandalism Data from Chicago

```{r}
policeDistricts <- 
  st_read("https://data.cityofchicago.org/api/geospatial/fthy-xz3r?method=export&format=GeoJSON") %>%
  st_transform('ESRI:102271') %>%
  dplyr::select(District = dist_num)
  
policeBeats <- 
  st_read("https://data.cityofchicago.org/api/geospatial/aerh-rz74?method=export&format=GeoJSON") %>%
  st_transform('ESRI:102271') %>%
  dplyr::select(District = beat_num)

bothPoliceUnits <- rbind(mutate(policeDistricts, Legend = "Police Districts"), 
                         mutate(policeBeats, Legend = "Police Beats"))

vandalism <- 
  read.socrata("https://data.cityofchicago.org/Public-Safety/Crimes-2017/d62x-nvdr") %>% 
    filter(Primary.Type == "CRIMINAL DAMAGE" & Description == "TO PROPERTY") %>%
    mutate(x = gsub("[()]", "", Location)) %>%
    separate(x,into= c("Y","X"), sep=",") %>%
    mutate(X = as.numeric(X),Y = as.numeric(Y)) %>% 
    na.omit() %>%
    st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant")%>%
    st_transform('ESRI:102271') %>% 
    distinct()

chicagoBoundary <- 
  st_read(file.path(root.dir,"/Chapter5/chicagoBoundary.geojson")) %>%
  st_transform('ESRI:102271') 
```

## visualizing point data

Plotting point data and density

> Our outcome of interest in this study is a prediction of vandalism sites accurate to actual reported vandalism sites. We think that data collected on vandalism of property could contain selection bias for several reasons. One factor that could play a role in the reporting of property damage would be property ownership, with homeowners assumed to be more likely to notice and report vandalism than absentee who landlords, who may not notice or care as much to invest and repair the damage. It should be noticed as well that we sourced our data from the Chicago Police, so the vandalism reports recorded were from police calls. Different socioeconomic and racial groups may have different views on reporting property damage to police, with people of color, especially black people being disproportionately affected by police brutality in America, it is possible that vandalism is underreported by people of color and renters.

```{r fig.width=6, fig.height=4}
# uses grid.arrange to organize independent plots
grid.arrange(ncol=2,
ggplot() + 
  geom_sf(data = chicagoBoundary) +
  geom_sf(data = vandalism, colour="red", size=0.1, show.legend = "point") +
  labs(title= "Vandalism, Chicago - 2017") +
  mapTheme(title_size = 14),

ggplot() + 
  geom_sf(data = chicagoBoundary, fill = "grey40") +
  stat_density2d(data = data.frame(st_coordinates(vandalism)), 
                 aes(X, Y, fill = ..level.., alpha = ..level..),
                 size = 0.01, bins = 40, geom = 'polygon') +
  scale_fill_viridis() +
  scale_alpha(range = c(0.00, 0.35), guide = FALSE) +
  labs(title = "Density of Vandalism") +
  mapTheme(title_size = 14) + theme(legend.position = "none"))
```

## Creating a fishnet grid


```{r}
## using {sf} to create the grid
## Note the `.[chicagoBoundary] %>% ` line. This is needed to clip the grid to our data
fishnet <- 
  st_make_grid(chicagoBoundary,
               cellsize = 500, 
               square = TRUE) %>%
  .[chicagoBoundary] %>%            # fast way to select intersecting polygons
  st_sf() %>%
  mutate(uniqueID = 1:n())


```

### Aggregate points to the fishnet

> In the figure below we map our data on 2017 property vandalism reports aggregated to a fishnet. A fishnet is a uniform grid of cells, in our case 500 meters each. By aggregating our data to uniformly sized shapes, we are avoiding a spatial error in which the aggregation is biased to certain boundaries. A common implementation of this spatial error is gerrymandering. Thus, our fishnet is a step to reduce bias in the prediction of vandalism.

```{r}
## add a value of 1 to each crime, sum them with aggregate
crime_net <- 
  dplyr::select(vandalism) %>% 
  mutate(countVandalism = 1) %>% 
  aggregate(., fishnet, sum) %>%
  mutate(countVandalism = replace_na(countVandalism, 0),
         uniqueID = 1:n(),
         cvID = sample(round(nrow(fishnet) / 24), 
                       size=nrow(fishnet), replace = TRUE))

ggplot() +
  geom_sf(data = crime_net, aes(fill = countVandalism), color = NA) +
  scale_fill_viridis() +
  labs(title = "Count of Vandalism for the fishnet") +
  mapTheme()

# For demo. requires updated mapview package
# xx <- mapview::mapview(crime_net, zcol = "countBurglaries")
# yy <- mapview::mapview(mutate(burglaries, ID = seq(1:n())))
# xx + yy
```

## Modeling Spatial Features

> We choose two spatial features in addition to those presented in our lab. One such feature was vacant buildings. We chose this feature because we assumed that vacant buildings are viewed as lacking ownership or protection and are more vulnerable to damage as a result. It would make sense under this logic that vacant buildings would be more likely to be vandalized than owned and maintained buildings. We used point data from the Chicago Police Portal on vacant building locations.

> Another variable we used was median household income from the American Community Survey. We chose this variable under the assumption that people with higher median income would be more likely to report vandalism because they likely wanted retribution for any damage of their property, which was probably higher in value than people with lower median household income. This data was aggregated by census tract, so unlike our vandalism and vacant buildings data, median household income was not in point form. Later we joined it to our fishnet with some manipulation. 

> These features may be problematic for two reasons. One is that median household income is not point data, and is aggregated to census block groups which are not uniform in size or population, which may introduce spatial bias. Another reason is that vacancies and median household income were chosen for opposite reasoning. We assume vandalism to occur most on property with no value (vacancies are usually listed as $0 property value) but to be reported most on properties with higher value. This presents a contradiction.


```{r}
## only pulling a single variable for our model to keep it simple
## using Socrata again
abandonCars <- 
  read.socrata("https://data.cityofchicago.org/Service-Requests/311-Service-Requests-Abandoned-Vehicles/3c9v-pnva") %>%
    mutate(year = substr(creation_date,1,4)) %>% filter(year == "2017") %>%
    dplyr::select(Y = latitude, X = longitude) %>%
    na.omit() %>%
    st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
    st_transform(st_crs(fishnet)) %>%
    mutate(Legend = "Abandoned_Cars")

## Neighborhoods to use in LOOCV in a bit
neighborhoods <- 
  st_read("https://raw.githubusercontent.com/blackmad/neighborhoods/master/chicago.geojson") %>%
  st_transform(st_crs(fishnet)) 

```

```{r}
#Pulling locations of vacant buildings
vacantBldgs <- 
  read.socrata("https://data.cityofchicago.org/Buildings/Vacant-and-Abandoned-Buildings-Violations/kc9i-wq85") %>%
    mutate(issued_date = substr(issued_date,1,4)) %>% filter(issued_date == "2017") %>%
    dplyr::select(Y = latitude, X = longitude) %>%
    na.omit() %>%
    st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
    st_transform(st_crs(fishnet)) %>%
    mutate(Legend = "Vacant_Bldgs")
```

```{r}
#Getting median HH income by Census tract
CHI_demo.sf <- 
  get_acs(geography = "block group", 
          variables = c("B19013_001E"), 
          year=2020, state=17, county=031, 
          geometry=TRUE, output="wide") %>%
  st_transform("ESRI:102271") %>%
  dplyr::select( -NAME, -B19013_001M)

CHI_demo.sf <-
  CHI_demo.sf %>%
  rename(HH_inc = B19013_001E)

```


#### How we aggregate a feature to our fishnet

```{r}
#Join count data
vars_net <- abandonCars %>%
  st_join(fishnet, join=st_within) %>%
  st_drop_geometry() %>%
  group_by(uniqueID, Legend) %>%
  summarize(count = n()) %>%
  left_join(fishnet, ., by = "uniqueID") %>%
  spread(Legend, count, fill=0) %>%
  dplyr::select(-`<NA>`) %>%
  ungroup()

vacant_agg <- vacantBldgs %>%
  st_join(fishnet, join=st_within) %>%
  st_drop_geometry() %>%
  group_by(uniqueID, Legend) %>%
  summarize(count = n()) %>%
  left_join(fishnet, ., by = "uniqueID") %>%
  spread(Legend, count, fill=0) %>%
  dplyr::select(-`<NA>`) %>%
  ungroup()

vars_net <- merge(as.data.frame(vars_net), as.data.frame(vacant_agg))
```

```{r}
#Join income data
vars_net <- st_as_sf(vars_net, sf_column_name = "geometry")

vars_net <- st_join(st_centroid(vars_net), CHI_demo.sf) %>%
  dplyr::select(-`GEOID`)
```

## Mapping Median Household Income data

```{r}
ggplot() +
      geom_sf(data = vars_net, aes(color=HH_inc)) +
      scale_color_viridis(name="Median Household Income") +
      labs(title="Median Household Income") +
  mapTheme()
```

## Nearest Neighbor Feature

> 

```{r}
# convenience to reduce length of function names.
st_c    <- st_coordinates
st_coid <- st_centroid

## create NN from abandoned cars and vacant buildings
vars_net <- vars_net %>%
    mutate(Abandoned_Cars.nn = nn_function(st_c(st_coid(vars_net)), st_c(abandonCars), k = 3)) %>% 
  mutate(vacantBldgsNn = nn_function(st_c(st_coid(vars_net)), st_c(vacantBldgs), k = 3))
```



```{r}
## Visualize the NN feature - abandoned cars
vars_net.long.nn <- 
  dplyr::select(vars_net, ends_with(".nn")) %>%
    gather(Variable, value, -geometry)

## Visualize the NN feature - vacant buildings
vars_net.longNn <- 
  dplyr::select(vars_net, ends_with("Nn")) %>%
    gather(Variable, value, -geometry)
```

## Join NN feature to our fishnet

Since the counts were aggregated to each cell by `uniqueID` we can use that to join the counts to the fishnet.

```{r}
## important to drop the geometry from joining features
final_net <-
  left_join(crime_net, st_drop_geometry(vars_net), by="uniqueID")

View(final_net)

grid.arrange(ncol=3,
ggplot() +
      geom_sf(data = final_net, aes(fill=HH_inc), color=NA) +
      scale_fill_viridis(name="Med HH Income") +
      labs(title="Median Household Income") +
      mapTheme(),          
ggplot() +
      geom_sf(data = final_net, aes(fill=Abandoned_Cars.nn), color=NA) +
      scale_fill_viridis(name="NN Distance") +
      labs(title="Abandoned Car NN Distance") +
      mapTheme(),
ggplot() +
      geom_sf(data = final_net, aes(fill=vacantBldgsNn), color=NA) +
      scale_fill_viridis(name="NN Distance") +
      labs(title="Vacant Building NN Distance") +
  mapTheme()
)

```

### Join in areal data


```{r}

final_net <-
  st_centroid(final_net) %>%
    st_join(dplyr::select(neighborhoods, name), by = "uniqueID") %>%
    st_join(dplyr::select(policeDistricts, District), by = "uniqueID") %>%
      st_drop_geometry() %>%
      left_join(dplyr::select(final_net, geometry, uniqueID)) %>%
      st_sf() %>%
  na.omit()

# for live demo
# mapview::mapview(final_net, zcol = "District")
```

## Local Moran's I for fishnet grid cells


```{r}
## generates warnings from PROJ issues
## {spdep} to make polygon to neighborhoods... 
final_net.nb <- poly2nb(as_Spatial(final_net), queen=TRUE)
## ... and neighborhoods to list of weights
final_net.weights <- nb2listw(final_net.nb, style="W", zero.policy=TRUE)

# print(final_net.weights, zero.policy=TRUE)
```

```{r}
## see ?localmoran
local_morans_ac <- localmoran(final_net$Abandoned_Cars, final_net.weights, zero.policy=TRUE) %>% 
  as.data.frame()

local_morans_vb <- localmoran(final_net$Vacant_Bldgs, final_net.weights, zero.policy=TRUE) %>% 
  as.data.frame()

# join local Moran's I results to fishnet
final_net.localMorans_ac <- 
  cbind(local_morans_ac, as.data.frame(final_net)) %>% 
  st_sf() %>%
  dplyr::select(Abandoned_Cars_Count = Abandoned_Cars, 
                Local_Morans_I = Ii, 
                P_Value = `Pr(z != E(Ii))`) %>%
  mutate(Significant_Hotspots = ifelse(P_Value <= 0.001, 1, 0)) %>%
  gather(Variable, Value, -geometry)



final_net.localMorans_vb <-
   cbind(local_morans_vb, as.data.frame(final_net)) %>% 
  st_sf() %>%
  dplyr::select(Vacant_Bldgs_Count = Vacant_Bldgs, 
                Local_Morans_I = Ii, 
                P_Value = `Pr(z != E(Ii))`) %>%
  mutate(Significant_Hotspots = ifelse(P_Value <= 0.001, 1, 0)) %>%
  gather(Variable, Value, -geometry)
  
```

### Plotting local Moran's I results

This is a complex code chunk - it's a loop which builds ggplots of local Moran's for each of your `vars`

> What does a significant hot spot tell us about the distribution of burglaries?

```{r}
## This is just for plotting
vars <- unique(final_net.localMorans_ac$Variable)
varList <- list()

for(i in vars){
  varList[[i]] <- 
    ggplot() +
      geom_sf(data = filter(final_net.localMorans_ac, Variable == i), 
              aes(fill = Value), colour=NA) +
      scale_fill_viridis(name="") +
      labs(title=i) +
      mapTheme(title_size = 14) + theme(legend.position="bottom")}

do.call(grid.arrange,c(varList, ncol = 4, top = "Local Morans I statistics, Vandalism"))
```

## Attempt to do the same thing with vacancies

```{r}
## This is just for plotting
vars <- unique(final_net.localMorans_vb$Variable)
varList <- list()

for(i in vars){
  varList[[i]] <- 
    ggplot() +
      geom_sf(data = filter(final_net.localMorans_vb, Variable == i), 
              aes(fill = Value), colour=NA) +
      scale_fill_viridis(name="") +
      labs(title=i) +
      mapTheme(title_size = 14) + theme(legend.position="bottom")}

do.call(grid.arrange,c(varList, ncol = 4, top = "Local Morans I statistics, Vandalism"))
```


##Scatter plot comparing independent and dependent variables

```{r}
#library(ggpmisc)
grid.arrange(ncol=2,
ggplot(final_net, aes(x=Vacant_Bldgs, y=countVandalism)) +
  geom_point(alpha=0.15) +
  labs(title="Vacant Buildings vs. Vandalism Reports per Grid Square") +
  ylab("# of Vandalism Reports") +
  xlab("# of Vacant Buildings") +
  geom_smooth(method='lm', formula= y~x, formula = my.formula),
ggplot(final_net, aes(x=HH_inc, y=countVandalism)) +
  geom_point(alpha=0.15) +
  labs(title="Household Income vs. Vandalism Reports per Grid Square") +
  ylab("# of Vandalism Reports") +
  xlab("Median HH Income") +
  geom_smooth(method='lm', formula= y~x))
```
##Histogram of our dependent variable
```{r}
grid.arrange(ncol=2,
ggplot(final_net, aes(countVandalism)) + 
  geom_histogram(color="black", fill="white"),
ggplot(final_net, aes(log(countVandalism))) + 
  geom_histogram(color="black", fill="white"))

#We could not find a transformation that made this variable normally-distributed, so we kept it as it is.
```


## Distance to Hot spot

Using NN distance to a hot spot location

```{r}
# generates warning from NN
final_net <- final_net %>% 
  mutate(abandoned.isSig = 
           ifelse(local_morans_ac[,5] <= 0.001, 1, 0)) %>%
  mutate(abandoned.isSig.dist = 
           nn_function(st_c(st_coid(final_net)),
                       st_c(st_coid(filter(final_net, 
                                           abandoned.isSig == 1))), 
                       k = 1))

## What does k = 1 represent?
```

> What does `k = 1` above mean in terms of measuring nearest neighbors?

### Plot NN distance to hot spot

```{r}
ggplot() +
      geom_sf(data = final_net, aes(fill=abandoned.isSig.dist), colour=NA) +
      scale_fill_viridis(name="NN Distance") +
      labs(title="Abandoned Car NN Distance") +
      mapTheme()
```

## Modeling and CV

Leave One Group Out CV on spatial features

```{r sults='hide'}

# View(crossValidate)

## define the variables we want
reg.ss.vars <- c("Abandoned_Cars.nn", "abandoned.isSig.dist", "vacantBldgsNn","HH_inc")

## RUN REGRESSIONS
reg.ss.spatialCV <- crossValidate(
  dataset = final_net,
  id = "name",                           
  dependentVariable = "countVandalism",
  indVariables = reg.ss.vars) %>%
    dplyr::select(cvID = name, countVandalism, Prediction, geometry)
```

```{r}
# calculate errors by NEIGHBORHOOD
error_by_reg_and_fold <- 
  reg.ss.spatialCV %>%
    group_by(cvID) %>% 
    summarize(Mean_Error = mean(Prediction - countVandalism, na.rm = T),
              MAE = mean(abs(Mean_Error), na.rm = T),
              SD_MAE = mean(abs(Mean_Error), na.rm = T)) %>%
  ungroup()

error_by_reg_and_fold %>% 
  arrange(desc(MAE))
error_by_reg_and_fold %>% 
  arrange(MAE)

## plot histogram of OOF (out of fold) errors
error_by_reg_and_fold %>%
  ggplot(aes(MAE)) + 
    geom_histogram(bins = 30, colour="black", fill = "#FDE725FF") +
  scale_x_continuous(breaks = seq(0, 11, by = 1)) + 
    labs(title="Distribution of MAE", subtitle = "LOGO-CV",
         x="Mean Absolute Error", y="Count") 
```
## K-Fold Cross Validation

```{r}
fitControl <- trainControl(method = "cv", number = 100)
set.seed(825)

reg.cv <- 
  train(countVandalism ~ ., data = st_drop_geometry(final_net) %>% 
                                dplyr::select(countVandalism, all_of(reg.ss.vars)), 
     method = "lm", trControl = fitControl, na.action = na.pass)

#reg.cv
#reg.cv$resample[1:5,]

library(data.table)

MAE.cv = data.table(MAE.cv= reg.cv$resample[,3])
```


## Mapping Error

```{r}
ggplot() +
  geom_sf(data=error_by_reg_and_fold, aes(fill=MAE),color=NA) +
  scale_fill_viridis(name="LOOCV MAE") +
      labs(title="MAE") +
      mapTheme()

#Summarizing MAE and SD for each regression.

stargazer(data=as.data.frame(error_by_reg_and_fold),type="text")

stargazer(data=MAE.cv,type="text")

```

## Density vs predictions

The `spatstat` function gets us kernal density estimates with varying search radii.

Note that the code here is *different* than in the book - it has been updated to keep up with changes in packages.

```{r}
# demo of kernel width
vandal_ppp <- as.ppp(st_coordinates(vandalism), W = st_bbox(final_net))
vandal_KD.1000 <- spatstat.core::density.ppp(vandal_ppp, 1000)
vandal_KD.1500 <- spatstat.core::density.ppp(vandal_ppp, 1500)
vandal_KD.2000 <- spatstat.core::density.ppp(vandal_ppp, 2000)
vandal_KD.df <- rbind(
  mutate(data.frame(rasterToPoints(mask(raster(vandal_KD.1000), as(neighborhoods, 'Spatial')))), Legend = "1000 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(vandal_KD.1500), as(neighborhoods, 'Spatial')))), Legend = "1500 Ft."),
  mutate(data.frame(rasterToPoints(mask(raster(vandal_KD.2000), as(neighborhoods, 'Spatial')))), Legend = "2000 Ft.")) 

vandal_KD.df$Legend <- factor(vandal_KD.df$Legend, levels = c("1000 Ft.", "1500 Ft.", "2000 Ft."))

ggplot(data=vandal_KD.df, aes(x=x, y=y)) +
  geom_raster(aes(fill=layer)) + 
  facet_wrap(~Legend) +
  coord_sf(crs=st_crs(final_net)) + 
  scale_fill_viridis(name="Density") +
  labs(title = "Kernel density with 3 different search radii") +
  mapTheme(title_size = 14)
```

```{r}

as.data.frame(vandal_KD.1000) %>%
  st_as_sf(coords = c("x", "y"), crs = st_crs(final_net)) %>%
  aggregate(., final_net, mean) %>%
   ggplot() +
     geom_sf(aes(fill=value)) +
     geom_sf(data = sample_n(vandalism, 1500), size = .5) +
     scale_fill_viridis(name = "Density") +
     labs(title = "Kernel density of 2017 vandalism") +
     mapTheme(title_size = 14)
```

## Get 2018 crime data

Let's see how our model performed relative to KD on the following year's data.

```{r}
vandalism18 <- 
  read.socrata("https://data.cityofchicago.org/Public-Safety/Crimes-2018/3i3m-jwuy") %>% 
  filter(Primary.Type == "CRIMINAL DAMAGE" & 
         Description == "TO PROPERTY") %>%
  mutate(x = gsub("[()]", "", Location)) %>%
  separate(x,into= c("Y","X"), sep=",") %>%
  mutate(X = as.numeric(X),
         Y = as.numeric(Y)) %>% 
  na.omit %>%
  st_as_sf(coords = c("X", "Y"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102271') %>% 
  distinct() %>%
  .[fishnet,]
```

```{r}

vandal_KDE_sum <- as.data.frame(vandal_KD.1000) %>%
  st_as_sf(coords = c("x", "y"), crs = st_crs(final_net)) %>%
  aggregate(., final_net, mean) 
kde_breaks <- classIntervals(vandal_KDE_sum$value, 
                             n = 5, "fisher")
vandal_KDE_sf <- vandal_KDE_sum %>%
  mutate(label = "Kernel Density",
         Risk_Category = classInt::findCols(kde_breaks),
         Risk_Category = case_when(
           Risk_Category == 5 ~ "5th",
           Risk_Category == 4 ~ "4th",
           Risk_Category == 3 ~ "3rd",
           Risk_Category == 2 ~ "2nd",
           Risk_Category == 1 ~ "1st")) %>%
  cbind(
    aggregate(
      dplyr::select(vandalism18) %>% mutate(vandalCount = 1), ., sum) %>%
    mutate(vandalCount = replace_na(vandalCount, 0))) %>%
  dplyr::select(label, Risk_Category, vandalCount)
```

Note that this is different from the book, where we pull a model out of a list of models we've created. For your homework, you'll be creating multiple models.

```{r}
ml_breaks <- classIntervals(reg.ss.spatialCV$Prediction, 
                             n = 5, "fisher")
vandal_risk_sf <-
  reg.ss.spatialCV %>%
  mutate(label = "Risk Predictions",
         Risk_Category =classInt::findCols(ml_breaks),
         Risk_Category = case_when(
           Risk_Category == 5 ~ "5th",
           Risk_Category == 4 ~ "4th",
           Risk_Category == 3 ~ "3rd",
           Risk_Category == 2 ~ "2nd",
           Risk_Category == 1 ~ "1st")) %>%
  cbind(
    aggregate(
      dplyr::select(vandalism18) %>% mutate(vandalCount = 1), ., sum) %>%
      mutate(vandalCount = replace_na(vandalCount, 0))) %>%
  dplyr::select(label,Risk_Category, vandalCount)
```

We don't do quite as well because we don't have very many features, but still pretty good.

```{r}
rbind(vandal_KDE_sf, vandal_risk_sf) %>%
  na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category, -geometry) %>%
  ggplot() +
    geom_sf(aes(fill = Risk_Category), colour = NA) +
    geom_sf(data = sample_n(vandalism18, 3000), size = .5, colour = "black") +
    facet_wrap(~label, ) +
    scale_fill_viridis(discrete = TRUE) +
    labs(title="Comparison of Kernel Density and Risk Predictions",
         subtitle="2017 vandalism risk predictions; 2018 vandalism") +
    mapTheme(title_size = 14)
```

```{r}
rbind(vandal_KDE_sf, vandal_risk_sf) %>%
  st_drop_geometry() %>%
  na.omit() %>%
  gather(Variable, Value, -label, -Risk_Category) %>%
  group_by(label, Risk_Category) %>%
  summarize(countVandalism = sum(Value)) %>%
  ungroup() %>%
  group_by(label) %>%
  mutate(Pcnt_of_test_set_crimes = countVandalism / sum(countVandalism)) %>%
    ggplot(aes(Risk_Category,Pcnt_of_test_set_crimes)) +
      geom_bar(aes(fill=label), position="dodge", stat="identity") +
      scale_fill_viridis(discrete = TRUE, name = "Model") +
      labs(title = "Risk prediction vs. Kernel density, 2018 vandalism",
           y = "% of Test Set Vandalism (per model)",
           x = "Risk Category") +
  theme_bw() +
      theme(axis.text.x = element_text(angle = 45, vjust = 0.5))
```

## Conclusion

We believe that this algorithm outpaces the status quo method and should be put into production. The status quo method is based on kernel density (KDE) - a.k.a, predicting future events based on past events. Observing our bar plot above, you can see that the KDE predicts a larger amount of spatial areas to contain occurrences of vandalism that are outside the 80th-percentile than the model we have created. Our risk prediction model follows more closely to a normal distribution when plotted over percentiles of events.

Our mean absolute error is around 2.6 vandalism reports. Considering that the mean number of vandalism reports per area is around 6, our error is not large enough to predict a false negative for an area. That means that law enforcement will be able to narrow down which areas to patrol for vandalism relatively well. While a useful model, in order to improve the predictions, the model could be changed to include more variables or transform our dependent variable to be normally distributed.
